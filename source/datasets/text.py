import os
import pickle
import psutil
import multiprocessing
import concurrent.futures
import tqdm

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

import tensorflow as tf
import pandas as pd

class TextDataset(object):

    '''
    Note: Docstrings out of date

    This class contains functions to create, save, load and manage
    text datasets. We are representing the random walks from the 
    node2vec graph as sentences, where nodes represent words, and 
    neighbours represent meaningful co-occurences. We use techniques
    from NLP to generate 'node embeddings' - continous representations
    of discrete nodes. 

    About: 
    This uses TensorFlow to perform all the data transformation 
    functions. We begin by creating a 'vocabulary'. This is the 
    entire set of nodes that exist in the graph. We use this to encode 
    nodes as integers that will be used in a look up table. We then use 
    these vectorised tokens to create a dataset of labelled skipgrams that 
    will be fed into the model. 

    Public API:
     - create_skipgram_dataset()        This loads random walks from the n2v
                                        graph and converts them into a skipgram
                                        dataset used to train an embedding model
     - data()                           Returns a tuned TFRecord dataset object
                                        which can be fed directly to a model
     - text_vectorizer()                Returns the text vectorisation layer

    '''

    cache = 'cache/tensorflow'

    def __init__(self, sample_filepath, skipgram_filepath, verbose=True):
        '''
        This initialises the TextDataset object with a sample and skipgram
        filepaths. 
         - sample_filepath: This is the path to random walks that were
            generated by sampling the node2vec graph
         - skipgram filepath: This is the path to the skipgrams that will be
            used to train the embedding model
        Note: both arguments must be provided to the constructor
        '''

        self.sample_filepath = sample_filepath
        self.skipgram_filepath = skipgram_filepath
        self.verbose = verbose


    def create_skipgram_dataset(self, window_size=2, negative_samples=5):
        '''
        This generates the skipgram dataset based on the given window size
        with the given number of negative samples. It creates a vocabulary 
        and text vbectorisation layer. It computes the skipgrams using 
        as much data parallelism as possible. It delegates one worker to be 
        a writer which writes records to a TFRecord dataset. The remaining 
        workers generate the skipgrams and pass them to the writer via a queue.
        '''

        # Check sample dataset exists and skipgram dataset does not
        sample_fp = self.sample_filepath
        skipgram_fp = self.skipgram_filepath

        assert os.path.exists(sample_fp), f'ERROR: Resource not found {os.path.split(sample_fp)[1]} - Have you generated these walks?'
        assert not os.path.exists(skipgram_fp), f'ERROR: Conflict, resource {os.path.split(skipgram_fp)[1]} already exists'

        # Create vocab
        self._create_vocabulary(sample_fp)

        # Load walks dataset as iterator
        chunksize = 1000
        length = len(list(self._load_walks_as_iterator(sample_fp, chunksize=chunksize)))
        sentences = self._load_walks_as_iterator(sample_fp, chunksize=chunksize)

        ## Data Parallelism: Skipgram generation
        queue = multiprocessing.Queue()
        response = multiprocessing.Queue()

        # Initiate tf writer in its own process
        writer = self._write_tf_record
        writer_p = multiprocessing.Process(target=writer, args=(skipgram_fp, queue, response))
        writer_p.start()

        # Process chunks 
        max_workers = multiprocessing.cpu_count() - 1 # -1 for the writer
        active_workers = { i: {} for i in range(max_workers)}
        func = self._create_skipgrams
        is_complete = False
        if (self.verbose):
            print(f'Generating skipgrams | {max_workers} workers')
            progress_bar = tqdm.tqdm(total=length)

        while not is_complete:
            
            spawn = 0
            spaces = []
            repeats = []

            for pid, process in active_workers.items(): # Loop through active workers
                if process:
                    worker = process['worker']  # Get the worker
                    data = process['data']  # Get the data assigned to the worker
                    if not worker.is_alive(): # If the worker has completed
                        spawn += 1  # We have room for a new worker
                        spaces.append(pid) # So free up their space
                        if not worker.exitcode == 0: # If the exit code was not 0, something went wrong, we need to repeat
                            repeats.append(data) # Repeat this chunk
                        worker.join() # Rest well, worker
                else:
                    spawn = max_workers
                    spaces = list(active_workers.keys())
            
            for repeat in repeats:
                worker = multiprocessing.Process(target=func, args=(repeat, window_size, negative_samples, queue))
                worker.start()
                pid = spaces.pop()
                active_workers[pid] = {
                    'worker': worker,
                    'data': repeat
                }
                spawn -= 1
                # print(f'Process started: id {pid} - restart')
            
            for _ in range(spawn):
                try:
                    chunk = next(sentences)
                except:
                    is_complete = True
                    [process['worker'].join() for _, process in active_workers.items()]
                    break
                worker = multiprocessing.Process(target=func, args=(chunk, window_size, negative_samples, queue))
                worker.start()
                pid = spaces.pop()
                active_workers[pid] = {
                    'worker': worker,
                    'data': chunk
                }
                spawn -= 1
                if self.verbose:
                    progress_bar.update(1)
                # print(f'Process started: id {pid}')   

        if self.verbose:
            progress_bar.close()             
        
        # Close the tf writer object
        queue.put({'type':'close'}) 

        has_completed = False
        while not has_completed: # Await response
            message = response.get()
            if message:
                if message['complete']:
                    has_completed = True
        writer_p.join() # Close 
        if self.verbose:
            print('Skipgram generation complete')


    def data(self):
        '''
        This function returns a tensorflow dataset which can be used to stream
        in the skipgrams and passed to a model for training. It performs 
        some optimisations to improve performance. A training example has the 
        form (x, y):
        (
            {
                'target': Tensor(<target word>),
                'context': Tensor(<context words>)
            },
            Tensor(<labels>)
        )
        '''

        dataset = tf.data.TFRecordDataset([self.skipgram_filepath])
        
        dataset = dataset.map(self._deserialize_example, num_parallel_calls=tf.data.AUTOTUNE)
        dataset = dataset.batch(1024).cache().prefetch(tf.data.AUTOTUNE)

        return dataset

    def text_vectorizer(self):
        '''
        Public method to return the text vectorisation layer. The returned layer
        is a callable. 
        '''

        sample_fp = self.sample_filepath

        self._create_vocabulary(sample_fp)

        return self.vocab_layer


    def _create_vocabulary(self, sample_fp):
        '''
        This creates or loads a vocabulary mapping object for a given 
        '''

        # Initialise the vocab mapping layer
        vocab_layer = tf.keras.layers.StringLookup()
        
        # Check if vocab already exists
        cache = os.path.join(self.cache, 'vocab')
        if not os.path.exists(cache):
            os.mkdir(cache)
        vocab_id = os.path.split(sample_fp)[1].split('.')[0] # 'path/to/file.csv' -> 'file'
        extension = 'pickle'
        
        filepath = os.path.join(cache, f'{vocab_id}.{extension}')

        vocab_exists = os.path.exists(filepath)

        # If exists load from cache
        if vocab_exists:
            
            vocab = self._load_pickle(filepath)
            vocab_layer.set_vocabulary(vocab)
        
        # Else create vocab and save to cache
        else:

            for chunk in self._load_walks_as_iterator(sample_fp):
                data = chunk.to_numpy()
                vocab_layer.adapt(data)
            
            vocab = vocab_layer.get_vocabulary()

            self._save_pickle(vocab, filepath)
        
        # Add mapping to object
        self.vocab_layer = vocab_layer
  

    def _load_walks_as_iterator(self, filepath, chunksize=1000):
        '''
        Loads a walks dataset in chunks, converts it to str
        and returns chunks as a generator. 
        '''

        with pd.read_csv(filepath, chunksize=chunksize) as reader:
            for chunk in reader:
                data = chunk.astype(str)
                yield data

    def _load_pickle(self, filepath):
        '''
        Load a pickle file from a given location
        '''
        try:

            with open(filepath, 'rb') as fp:
                data = pickle.load(fp)
            
            return data
        
        except Exception as e:

            print(f'ERROR: {e}')
            raise

    def _save_pickle(self, data, filepath):
        '''
        Load a pickle file from a given location
        '''
        try:

            with open(filepath, 'wb') as fp:
                pickle.dump(data, fp)
        
        except Exception as e:

            print(f'ERROR: {e}')
            raise        
    
    def _write_tf_record(self, filepath, queue, response):
        '''
        This function is responsible for writing serialised tf examples 
        to a tf records dataset. It is designed to be the single writer
        in a system designed to produce records in parallel. Producers
        enqueue message objects in the queue (passed as a param), which 
        has the structure:
        {
            'type': 'write' | 'close',
            'payload': [data] | None
        } 
        When all the processes have been submitted the connection is 
        closed by the caller. The function then confirms it has finished 
        by submitting a message to the response queue. 

        '''
        with tf.io.TFRecordWriter(filepath) as file_writer:

            listening = True

            while listening:

                message = queue.get()


                if not message: # Queue is empty
                    continue

                if message['type'] == 'write': # Write to file
                    for record in message['payload']:
                        file_writer.write(record)
                
                elif message['type'] == 'close': # Close writer
                    listening = False
        
        # Confirm has finished
        response.put({'complete':True})

    def _create_skipgrams(self, sentences, window_size, negative_samples, queue, i=None):
        '''
        This function generates the skipgrams which are used to train the embedding models. 
        It takes a pandas dataframe and vectorises each of the walks. Once each node has 
        been vectorised, it creates skipgrams for each line. One training example consists of 
        a ({target, context}, label) tuple, where target is size 1, and context and label are
        of size negative_samples + 1. This trainig example is then serialized into binary, 
        and enqueued for the writer to add to the file. 
        '''

        if i and self.verbose: 
            print(f'Starting process #{i}')

        vocab_size = self.vocab_layer.vocabulary_size()

        # Convert Pandas DF to NP array
        lines = sentences.to_numpy()

        # Vectorise the words
        vectorized_data = self.vocab_layer(lines)

        # Make skipgrams
        for line in vectorized_data:

            positive_skipgrams = self._create_positive_skipgrams(line, window_size, vocab_size)

            for positive_skipgram in positive_skipgrams:

                negative_skipgrams = self._create_negative_skipgrams(positive_skipgram, vocab_size, negative_samples)
                target, context, label = self._create_example(positive_skipgram, negative_skipgrams, negative_samples)

                proto = self._serialize_example(target, context, label)

                # Add to writer queue
                queue.put({'type':'write', 'payload':[proto]})

    
    def _create_positive_skipgrams(self, line, window_size, vocab_size):
        '''
        This generates positive skipgrams for a given sequence for a given
        window size
        '''

        positive_skipgrams, _ = tf.keras.preprocessing.sequence.skipgrams(
            line, 
            vocabulary_size=vocab_size,
            window_size=window_size,
            negative_samples=0
        )

        return positive_skipgrams

    def _create_negative_skipgrams(self, positive_skipgram, vocab_size, negative_samples):
        '''
        This generates negative skipgrams for a given positive skipgram by sampling from 
        a log uniform distribution
        '''

        target_word, context_word = positive_skipgram

        context_class = tf.reshape(tf.constant(context_word, dtype='int64'), (1, 1))

        negative_examples, _, _ = tf.random.log_uniform_candidate_sampler(
            true_classes=context_class,
            num_true=1,
            num_sampled=negative_samples,
            unique=True,
            range_max=vocab_size,
            name='negative_sampling'
        )

        return negative_examples

    def _create_example(self, positive_skipgram, negative_skipgrams, negative_samples):
        '''
        This creates the training examples by concatenating the positive and negative
        skipgrams into one training example and producing the corresponding labels. 
        '''
        target_word, context_word = positive_skipgram

        context_class = tf.reshape(tf.constant(context_word, dtype='int64'), (1, 1))

        negative_skipgrams = tf.expand_dims(negative_skipgrams, 1)

        # Create a batch of context words
        axis = 0
        context_words = tf.concat([context_class, negative_skipgrams], axis)

        # Create a batch of labels
        labels = tf.constant([1] + [0] * negative_samples, dtype='int64')

        # Reshape tensors
        target = tf.squeeze(target_word)
        context = tf.squeeze(context_words)
        label = tf.squeeze(labels)

        return target, context, label

    def _serialize_example(self, target, context, label):
        '''
        This function takes the components of a skipgram training example
        and serialises them into a tensorflow training proto.
        '''

        feature = {
            'target': self._tensor_feature(target),
            'context': self._tensor_feature(context),
            'label': self._tensor_feature(label),
        }

        proto = tf.train.Example(features=tf.train.Features(feature=feature))

        return proto.SerializeToString()

    def _int64_feature(self, feature):
        '''
        Serialise an integer into a proto feature
        '''
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[feature]))

    def _tensor_feature(self, tensor):
        '''
        Serialise a tensor into a proto feature
        '''

        serialized_tensor = tf.io.serialize_tensor(tensor)

        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_tensor.numpy()]))


    def _deserialize_example(self, record):
        '''
        Turn a serialised tf record into a training example which can be fed into a model.
        This begins by parsing the proto string into an example. It then parses each 
        of the individual tensors to produce their numerical values. Finally, it 
        arranges these into x and y variables which will be fed to the model and returns
        the final training example as a tuple. 
        '''


        description = {
            'target': tf.io.FixedLenFeature([], tf.string),
            'context': tf.io.FixedLenFeature([], tf.string),
            'label': tf.io.FixedLenFeature([], tf.string),
        }

        example = tf.io.parse_single_example(record, description)

        x = {

            'target': tf.io.parse_tensor(example['target'], out_type=tf.int64),
            'context': tf.io.parse_tensor(example['context'], out_type=tf.int64)

        }       

        y = tf.io.parse_tensor(example['label'], out_type=tf.int64)
        
        return (x, y)




if __name__ == "__main__":
    walks_fp = 'data/processed/walks/3ff0cd3d00ebf7.csv'
    skipgram_fp = 'data/processed/skipgrams/3ff0cd3d00ebf7'
    td = TextDataset(walks_fp, skipgram_fp)

    # td.create_skipgram_dataset()
    data = td.data()
    for i in data.take(5):
        print(i)
        break
